{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from sklearn.model_selection import cross_val_score\n",
    " from sklearn.datasets import make_blobs\n",
    " from sklearn.ensemble import RandomForestClassifier\n",
    " from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    " X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "...     random_state=0)\n",
    "\n",
    " clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "...     random_state=0)\n",
    "scores = cross_val_score(clf, X, y)\n",
    " scores.mean()                             \n",
    "0.97...\n",
    "\n",
    " clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "...     min_samples_split=2, random_state=0)\n",
    " scores = cross_val_score(clf, X, y)\n",
    " scores.mean()                             \n",
    "0.999...\n",
    "\n",
    " clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "...     min_samples_split=2, random_state=0)\n",
    " scores = cross_val_score(clf, X, y)\n",
    " scores.mean() > 0.999\n",
    "True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    " iris = load_iris()\n",
    " clf = AdaBoostClassifier(n_estimators=100)\n",
    " scores = cross_val_score(clf, iris.data, iris.target)\n",
    " scores.mean()                             \n",
    "0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    " X_train, X_test = X[:2000], X[2000:]\n",
    " y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    " clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)                 \n",
    "0.913..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " import numpy as np\n",
    " from sklearn.metrics import mean_squared_error\n",
    " from sklearn.datasets import make_friedman1\n",
    " from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    " X_train, X_test = X[:200], X[200:]\n",
    " y_train, y_test = y[:200], y[200:]\n",
    " est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    " mean_squared_error(y_test, est.predict(X_test))    \n",
    "5.00..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    " _ = est.fit(X_train, y_train) # fit additional 100 trees to est\n",
    " mean_squared_error(y_test, est.predict(X_test))    \n",
    "3.84..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
